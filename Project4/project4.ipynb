{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1sG8s-Fl9hc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device} device')"
      ],
      "metadata": {
        "id": "efELulD5nMky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    #transforms.RandomRotation(45),\n",
        "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "\n",
        "])\n",
        "\n",
        "# Download CIFAR-10 data\n",
        "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                             download=True, transform=transform)\n",
        "\n",
        "# Split the training data into training and validation subsets\n",
        "train_size = int(0.8 * len(full_trainset))  # Use 80% for training\n",
        "valid_size = len(full_trainset) - train_size  # Use the rest for validation\n",
        "\n",
        "trainset, validset = torch.utils.data.random_split(full_trainset, [train_size, valid_size])\n",
        "\n",
        "# Create data loaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=32,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "Jhs5NF8_mAci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of training images: ', len(trainset))\n",
        "print('Number of validation images: ', len(validset))\n",
        "print('Number of test images: ', len(testset))\n",
        "\n",
        "#Print classes and index\n",
        "print(testloader.dataset.class_to_idx)\n",
        "\n",
        "def get_name_from_index(index):\n",
        "    for name, num in testloader.dataset.class_to_idx.items():\n",
        "        if num == index:\n",
        "            return name\n"
      ],
      "metadata": {
        "id": "Ch8hw5t2mBjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGStyleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGGStyleModel, self).__init__()\n",
        "\n",
        "        # Define convolutional layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.4),\n",
        "        )\n",
        "\n",
        "        # Define fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4, 512),  # assuming input size is 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10),  # assuming a classification task with 10 classes\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output from conv layers\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "OBRauYqpmDFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = VGGStyleModel().to(device)  # Move model to GPU\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "8NjjzwhDmF2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "EPOCHS = 20\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "valid_losses = []\n",
        "val_accs = []\n",
        "\n",
        "#best_valid_loss = float('inf')  # Initialize with infinity to ensure any loss is smaller\n",
        "#best_model_wts = None  # To store best model weights\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_train = 0  # To calculate training accuracy\n",
        "\n",
        "    for inputs, labels in tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", unit=\"batch\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        #print(f\"inputs: {inputs[0].shape}\")\n",
        "        #print(f\"labels: {labels[0]}\")\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze()\n",
        "        #print(f\"outputs: {outputs.shape}\")\n",
        "        #print one output\n",
        "        #print(f\"outputs: {outputs}\")\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        #print(f\"preds: {preds}\")\n",
        "\n",
        "        correct_train += (preds == labels).sum().item()\n",
        "\n",
        "\n",
        "    train_losses.append(train_loss/len(trainloader))\n",
        "    train_acc = correct_train / len(trainloader.dataset)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct_valid = 0  # To calculate validation accuracy\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct_valid += (preds == labels).sum().item()\n",
        "\n",
        "    valid_losses.append(valid_loss/len(validloader))\n",
        "    valid_acc = correct_valid / len(validloader.dataset)\n",
        "    val_accs.append(valid_acc)\n",
        "\n",
        "    # Save the model if it has a lower validation loss than the best model seen so far\n",
        "    #if valid_losses[-1] < best_valid_loss:\n",
        "    #    best_valid_loss = valid_losses[-1]\n",
        "    #    best_model_wts = model.state_dict().copy()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_losses[-1]:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
        "\n",
        "# Load the best model weights\n",
        "#model.load_state_dict(best_m\n",
        "#Epoch 10/20 - Train Loss: 0.2605, Train Acc: 0.9077, Valid Loss: 0.9871, Valid Acc: 0.7476"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnoEQ6FGmGYc",
        "outputId": "ced4020a-1d9e-47d6-c69b-3fa81f12c2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 1250/1250 [00:27<00:00, 44.65batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Train Loss: 1.7504, Train Acc: 0.3492, Valid Loss: 1.3071, Valid Acc: 0.5212\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 1250/1250 [00:19<00:00, 65.62batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 - Train Loss: 1.3286, Train Acc: 0.5194, Valid Loss: 1.1306, Valid Acc: 0.5937\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 1250/1250 [00:19<00:00, 65.74batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20 - Train Loss: 1.1506, Train Acc: 0.5916, Valid Loss: 0.9897, Valid Acc: 0.6493\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 1250/1250 [00:19<00:00, 64.51batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20 - Train Loss: 1.0566, Train Acc: 0.6268, Valid Loss: 0.9308, Valid Acc: 0.6666\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 1250/1250 [00:20<00:00, 60.10batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20 - Train Loss: 0.9818, Train Acc: 0.6562, Valid Loss: 0.8603, Valid Acc: 0.6960\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 1250/1250 [00:20<00:00, 61.88batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/20 - Train Loss: 0.9382, Train Acc: 0.6709, Valid Loss: 0.8207, Valid Acc: 0.7167\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 1250/1250 [00:19<00:00, 63.87batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/20 - Train Loss: 0.8952, Train Acc: 0.6869, Valid Loss: 0.7852, Valid Acc: 0.7276\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 1250/1250 [00:19<00:00, 65.45batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/20 - Train Loss: 0.8736, Train Acc: 0.6955, Valid Loss: 0.7793, Valid Acc: 0.7275\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 1250/1250 [00:18<00:00, 67.12batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/20 - Train Loss: 0.8467, Train Acc: 0.7048, Valid Loss: 0.7526, Valid Acc: 0.7361\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 1250/1250 [00:19<00:00, 63.88batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20 - Train Loss: 0.8233, Train Acc: 0.7147, Valid Loss: 0.7802, Valid Acc: 0.7305\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 1250/1250 [00:19<00:00, 64.55batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/20 - Train Loss: 0.8061, Train Acc: 0.7224, Valid Loss: 0.7198, Valid Acc: 0.7531\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 1250/1250 [00:18<00:00, 67.08batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/20 - Train Loss: 0.7960, Train Acc: 0.7258, Valid Loss: 0.7109, Valid Acc: 0.7539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 1250/1250 [00:18<00:00, 65.87batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 - Train Loss: 0.7748, Train Acc: 0.7317, Valid Loss: 0.7169, Valid Acc: 0.7539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 1250/1250 [00:18<00:00, 67.80batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 - Train Loss: 0.7657, Train Acc: 0.7380, Valid Loss: 0.7376, Valid Acc: 0.7485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20:  49%|████▉     | 613/1250 [00:09<00:13, 48.70batch/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"model_state_dict.pth\"\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "iAEzVXw5oyRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2,1)\n",
        "axs[0].plot(train_losses, label='Training loss')\n",
        "axs[0].plot(valid_losses, label='Validation loss')\n",
        "axs[0].legend()\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_title('Loss vs. Epoch')\n",
        "\n",
        "axs[1].plot(train_accs, label='Training Accuracy')\n",
        "axs[1].plot(val_accs, label='Validation Accuracy')\n",
        "axs[1].legend()\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].set_title('Accuracy vs. Epoch')\n"
      ],
      "metadata": {
        "id": "quj8RKaJwuFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_prection(testloader, model):\n",
        "    random_idx = random.randint(0, len(testloader.dataset)-1)\n",
        "    sample_img, sample_label = testloader.dataset[random_idx]\n",
        "\n",
        "    # Get the model prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = model(sample_img.unsqueeze(0).to(device))\n",
        "        print(f\"pred: {pred}\")\n",
        "        pred_class = torch.argmax(pred, dim=1)\n",
        "        print(f\"pred_class: {pred_class}\")\n",
        "\n",
        "    # Visualize the sample image\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axs[0].imshow(sample_img.permute(1, 2, 0))\n",
        "    axs[1].imshow(sample_img.permute(1, 2, 0))\n",
        "    axs[1].set_title(f\"True Label: {get_name_from_index(sample_label)}\")\n",
        "    axs[0].set_title(f\"Predicted Label: {get_name_from_index(pred_class)}\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_prection(testloader, model)\n"
      ],
      "metadata": {
        "id": "VAl9aBlPmHkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}